{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"KeyBERT \u00b6 KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document. About the Project \u00b6 Although that are already many methods available for keyword generation (e.g., Rake , YAKE! , TF-IDF, etc.) I wanted to create a very basic, but powerful method for extracting keywords and keyphrases. This is where KeyBERT comes in! Which uses BERT-embeddings and simple cosine similarity to find the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. KeyBERT is by no means unique and is created as a quick and easy method for creating keywords and keyphrases. Although there are many great papers and solutions out there that use BERT-embeddings (e.g., 1 , 2 , 3 , ), I could not find a BERT-based solution that did not have to be trained from scratch and could be used for beginners ( correct me if I'm wrong! ). Thus, the goal was a pip install keybert and at most 3 lines of code in usage. NOTE : If you use MMR to select the candidates instead of simple cosine similarity, this repo is essentially a simplified implementation of EmbedRank with BERT-embeddings. Installation \u00b6 Installation can be done using pypi : pip install keybert To use Flair embeddings, install KeyBERT as follows: pip install keybert [ flair ] Usage \u00b6 The most minimal example can be seen below for the extraction of keywords: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" model = KeyBERT ( 'distilbert-base-nli-mean-tokens' ) You can set keyphrase_length to set the length of the resulting keyphras: >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 1 )) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] To extract keyphrases, simply set keyphrase_ngram_range to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases: >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 2 )) [( 'learning algorithm' , 0.6978 ), ( 'machine learning' , 0.6305 ), ( 'supervised learning' , 0.5985 ), ( 'algorithm analyzes' , 0.5860 ), ( 'learning function' , 0.5850 )]","title":"Home"},{"location":"index.html#keybert","text":"KeyBERT is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document.","title":"KeyBERT"},{"location":"index.html#about-the-project","text":"Although that are already many methods available for keyword generation (e.g., Rake , YAKE! , TF-IDF, etc.) I wanted to create a very basic, but powerful method for extracting keywords and keyphrases. This is where KeyBERT comes in! Which uses BERT-embeddings and simple cosine similarity to find the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. KeyBERT is by no means unique and is created as a quick and easy method for creating keywords and keyphrases. Although there are many great papers and solutions out there that use BERT-embeddings (e.g., 1 , 2 , 3 , ), I could not find a BERT-based solution that did not have to be trained from scratch and could be used for beginners ( correct me if I'm wrong! ). Thus, the goal was a pip install keybert and at most 3 lines of code in usage. NOTE : If you use MMR to select the candidates instead of simple cosine similarity, this repo is essentially a simplified implementation of EmbedRank with BERT-embeddings.","title":"About the Project"},{"location":"index.html#installation","text":"Installation can be done using pypi : pip install keybert To use Flair embeddings, install KeyBERT as follows: pip install keybert [ flair ]","title":"Installation"},{"location":"index.html#usage","text":"The most minimal example can be seen below for the extraction of keywords: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" model = KeyBERT ( 'distilbert-base-nli-mean-tokens' ) You can set keyphrase_length to set the length of the resulting keyphras: >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 1 )) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] To extract keyphrases, simply set keyphrase_ngram_range to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases: >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 2 )) [( 'learning algorithm' , 0.6978 ), ( 'machine learning' , 0.6305 ), ( 'supervised learning' , 0.5985 ), ( 'algorithm analyzes' , 0.5860 ), ( 'learning function' , 0.5850 )]","title":"Usage"},{"location":"api/keybert.html","text":"KeyBERT \u00b6 \u00b6 A minimal method for keyword extraction with BERT The keyword extraction is done by finding the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document. __init__ ( self , model = 'distilbert-base-nli-mean-tokens' ) special \u00b6 KeyBERT initialization Parameters: Name Type Description Default model Union[str, sentence_transformers.SentenceTransformer.SentenceTransformer, flair.embeddings.document.DocumentEmbeddings, flair.embeddings.token.TokenEmbeddings] Use a custom embedding model. You can pass in a string related to one of the following models: https://www.sbert.net/docs/pretrained_models.html You can also pass in a SentenceTransformer() model or a Flair DocumentEmbedding model. 'distilbert-base-nli-mean-tokens' Source code in keybert\\model.py def __init__ ( self , model : Union [ str , SentenceTransformer , DocumentEmbeddings , TokenEmbeddings ] = 'distilbert-base-nli-mean-tokens' ): \"\"\" KeyBERT initialization Arguments: model: Use a custom embedding model. You can pass in a string related to one of the following models: https://www.sbert.net/docs/pretrained_models.html You can also pass in a SentenceTransformer() model or a Flair DocumentEmbedding model. \"\"\" self . model = self . _select_embedding_model ( model ) extract_keywords ( self , docs , keyphrase_ngram_range = ( 1 , 1 ), stop_words = 'english' , top_n = 5 , min_df = 1 , use_maxsum = False , use_mmr = False , diversity = 0.5 , nr_candidates = 20 , vectorizer = None ) \u00b6 Extract keywords/keyphrases !!! note I would advise you to iterate over single documents as they will need the least amount of memory. Even though this is slower, you are not likely to run into memory errors. Multiple Documents: There is an option to extract keywords for multiple documents that is faster than extraction for multiple single documents. However ... this method assumes that you can keep the word embeddings for all words in the vocabulary in memory which might be troublesome . I would advise against using this option and simply iterating over documents instead if you have limited hardware . Parameters: Name Type Description Default docs Union[str, List[str]] The document(s) for which to extract keywords/keyphrases required keyphrase_ngram_range Tuple[int, int] Length, in words, of the extracted keywords/keyphrases (1, 1) stop_words Union[str, List[str]] Stopwords to remove from the document 'english' top_n int Return the top n keywords/keyphrases 5 min_df int Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted 1 use_maxsum bool Whether to use Max Sum Similarity for the selection of keywords/keyphrases False use_mmr bool Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases False diversity float The diversity of the results between 0 and 1 if use_mmr is set to True 0.5 nr_candidates int The number of candidates to consider if use_maxsum is set to True 20 vectorizer CountVectorizer Pass in your own CountVectorizer from scikit-learn None Returns: Type Description Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]] keywords: the top n keywords for a document with their respective distances to the input document Source code in keybert\\model.py def extract_keywords ( self , docs : Union [ str , List [ str ]], keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = 'english' , top_n : int = 5 , min_df : int = 1 , use_maxsum : bool = False , use_mmr : bool = False , diversity : float = 0.5 , nr_candidates : int = 20 , vectorizer : CountVectorizer = None ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\" Extract keywords/keyphrases NOTE: I would advise you to iterate over single documents as they will need the least amount of memory. Even though this is slower, you are not likely to run into memory errors. Multiple Documents: There is an option to extract keywords for multiple documents that is faster than extraction for multiple single documents. However...this method assumes that you can keep the word embeddings for all words in the vocabulary in memory which might be troublesome. I would advise against using this option and simply iterating over documents instead if you have limited hardware. Arguments: docs: The document(s) for which to extract keywords/keyphrases keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases stop_words: Stopwords to remove from the document top_n: Return the top n keywords/keyphrases min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted use_maxsum: Whether to use Max Sum Similarity for the selection of keywords/keyphrases use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases diversity: The diversity of the results between 0 and 1 if use_mmr is set to True nr_candidates: The number of candidates to consider if use_maxsum is set to True vectorizer: Pass in your own CountVectorizer from scikit-learn Returns: keywords: the top n keywords for a document with their respective distances to the input document \"\"\" if isinstance ( docs , str ): return self . _extract_keywords_single_doc ( docs , keyphrase_ngram_range , stop_words , top_n , use_maxsum , use_mmr , diversity , nr_candidates , vectorizer ) elif isinstance ( docs , list ): warnings . warn ( \"Although extracting keywords for multiple documents is faster \" \"than iterating over single documents, it requires significantly more memory \" \"to hold all word embeddings. Use this at your own discretion!\" ) return self . _extract_keywords_multiple_docs ( docs , keyphrase_ngram_range , stop_words , top_n , min_df , vectorizer )","title":"KeyBERT"},{"location":"api/keybert.html#keybert","text":"","title":"KeyBERT"},{"location":"api/keybert.html#keybert.model.KeyBERT","text":"A minimal method for keyword extraction with BERT The keyword extraction is done by finding the sub-phrases in a document that are the most similar to the document itself. First, document embeddings are extracted with BERT to get a document-level representation. Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document.","title":"keybert.model.KeyBERT"},{"location":"api/keybert.html#keybert.model.KeyBERT.__init__","text":"KeyBERT initialization Parameters: Name Type Description Default model Union[str, sentence_transformers.SentenceTransformer.SentenceTransformer, flair.embeddings.document.DocumentEmbeddings, flair.embeddings.token.TokenEmbeddings] Use a custom embedding model. You can pass in a string related to one of the following models: https://www.sbert.net/docs/pretrained_models.html You can also pass in a SentenceTransformer() model or a Flair DocumentEmbedding model. 'distilbert-base-nli-mean-tokens' Source code in keybert\\model.py def __init__ ( self , model : Union [ str , SentenceTransformer , DocumentEmbeddings , TokenEmbeddings ] = 'distilbert-base-nli-mean-tokens' ): \"\"\" KeyBERT initialization Arguments: model: Use a custom embedding model. You can pass in a string related to one of the following models: https://www.sbert.net/docs/pretrained_models.html You can also pass in a SentenceTransformer() model or a Flair DocumentEmbedding model. \"\"\" self . model = self . _select_embedding_model ( model )","title":"__init__()"},{"location":"api/keybert.html#keybert.model.KeyBERT.extract_keywords","text":"Extract keywords/keyphrases !!! note I would advise you to iterate over single documents as they will need the least amount of memory. Even though this is slower, you are not likely to run into memory errors. Multiple Documents: There is an option to extract keywords for multiple documents that is faster than extraction for multiple single documents. However ... this method assumes that you can keep the word embeddings for all words in the vocabulary in memory which might be troublesome . I would advise against using this option and simply iterating over documents instead if you have limited hardware . Parameters: Name Type Description Default docs Union[str, List[str]] The document(s) for which to extract keywords/keyphrases required keyphrase_ngram_range Tuple[int, int] Length, in words, of the extracted keywords/keyphrases (1, 1) stop_words Union[str, List[str]] Stopwords to remove from the document 'english' top_n int Return the top n keywords/keyphrases 5 min_df int Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted 1 use_maxsum bool Whether to use Max Sum Similarity for the selection of keywords/keyphrases False use_mmr bool Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases False diversity float The diversity of the results between 0 and 1 if use_mmr is set to True 0.5 nr_candidates int The number of candidates to consider if use_maxsum is set to True 20 vectorizer CountVectorizer Pass in your own CountVectorizer from scikit-learn None Returns: Type Description Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]] keywords: the top n keywords for a document with their respective distances to the input document Source code in keybert\\model.py def extract_keywords ( self , docs : Union [ str , List [ str ]], keyphrase_ngram_range : Tuple [ int , int ] = ( 1 , 1 ), stop_words : Union [ str , List [ str ]] = 'english' , top_n : int = 5 , min_df : int = 1 , use_maxsum : bool = False , use_mmr : bool = False , diversity : float = 0.5 , nr_candidates : int = 20 , vectorizer : CountVectorizer = None ) -> Union [ List [ Tuple [ str , float ]], List [ List [ Tuple [ str , float ]]]]: \"\"\" Extract keywords/keyphrases NOTE: I would advise you to iterate over single documents as they will need the least amount of memory. Even though this is slower, you are not likely to run into memory errors. Multiple Documents: There is an option to extract keywords for multiple documents that is faster than extraction for multiple single documents. However...this method assumes that you can keep the word embeddings for all words in the vocabulary in memory which might be troublesome. I would advise against using this option and simply iterating over documents instead if you have limited hardware. Arguments: docs: The document(s) for which to extract keywords/keyphrases keyphrase_ngram_range: Length, in words, of the extracted keywords/keyphrases stop_words: Stopwords to remove from the document top_n: Return the top n keywords/keyphrases min_df: Minimum document frequency of a word across all documents if keywords for multiple documents need to be extracted use_maxsum: Whether to use Max Sum Similarity for the selection of keywords/keyphrases use_mmr: Whether to use Maximal Marginal Relevance (MMR) for the selection of keywords/keyphrases diversity: The diversity of the results between 0 and 1 if use_mmr is set to True nr_candidates: The number of candidates to consider if use_maxsum is set to True vectorizer: Pass in your own CountVectorizer from scikit-learn Returns: keywords: the top n keywords for a document with their respective distances to the input document \"\"\" if isinstance ( docs , str ): return self . _extract_keywords_single_doc ( docs , keyphrase_ngram_range , stop_words , top_n , use_maxsum , use_mmr , diversity , nr_candidates , vectorizer ) elif isinstance ( docs , list ): warnings . warn ( \"Although extracting keywords for multiple documents is faster \" \"than iterating over single documents, it requires significantly more memory \" \"to hold all word embeddings. Use this at your own discretion!\" ) return self . _extract_keywords_multiple_docs ( docs , keyphrase_ngram_range , stop_words , top_n , min_df , vectorizer )","title":"extract_keywords()"},{"location":"api/maxsum.html","text":"Max Sum Similarity \u00b6 \u00b6 Calculate Max Sum Distance for extraction of keywords We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. !!! note This is O(n^2) and therefore not advised if you use a large top_n Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return required nr_candidates int The number of candidates to consider required Returns: Type Description List[Tuple[str, float]] List[Tuple[str, float]]: The selected keywords/keyphrases with their distances Source code in keybert\\maxsum.py def max_sum_similarity ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int , nr_candidates : int ) -> List [ Tuple [ str , float ]]: \"\"\" Calculate Max Sum Distance for extraction of keywords We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. NOTE: This is O(n^2) and therefore not advised if you use a large top_n Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return nr_candidates: The number of candidates to consider Returns: List[Tuple[str, float]]: The selected keywords/keyphrases with their distances \"\"\" if nr_candidates < top_n : raise Exception ( \"Make sure that the number of candidates exceeds the number \" \"of keywords to return.\" ) # Calculate distances and extract keywords distances = cosine_similarity ( doc_embedding , word_embeddings ) distances_words = cosine_similarity ( word_embeddings , word_embeddings ) # Get 2*top_n words as candidates based on cosine similarity words_idx = list ( distances . argsort ()[ 0 ][ - nr_candidates :]) words_vals = [ words [ index ] for index in words_idx ] candidates = distances_words [ np . ix_ ( words_idx , words_idx )] # Calculate the combination of words that are the least similar to each other min_sim = 100_000 candidate = None for combination in itertools . combinations ( range ( len ( words_idx )), top_n ): sim = sum ([ candidates [ i ][ j ] for i in combination for j in combination if i != j ]) if sim < min_sim : candidate = combination min_sim = sim return [( words_vals [ idx ], round ( float ( distances [ 0 ][ idx ]), 4 )) for idx in candidate ]","title":"MaxSum"},{"location":"api/maxsum.html#max-sum-similarity","text":"","title":"Max Sum Similarity"},{"location":"api/maxsum.html#keybert.maxsum.max_sum_similarity","text":"Calculate Max Sum Distance for extraction of keywords We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. !!! note This is O(n^2) and therefore not advised if you use a large top_n Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return required nr_candidates int The number of candidates to consider required Returns: Type Description List[Tuple[str, float]] List[Tuple[str, float]]: The selected keywords/keyphrases with their distances Source code in keybert\\maxsum.py def max_sum_similarity ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int , nr_candidates : int ) -> List [ Tuple [ str , float ]]: \"\"\" Calculate Max Sum Distance for extraction of keywords We take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. NOTE: This is O(n^2) and therefore not advised if you use a large top_n Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return nr_candidates: The number of candidates to consider Returns: List[Tuple[str, float]]: The selected keywords/keyphrases with their distances \"\"\" if nr_candidates < top_n : raise Exception ( \"Make sure that the number of candidates exceeds the number \" \"of keywords to return.\" ) # Calculate distances and extract keywords distances = cosine_similarity ( doc_embedding , word_embeddings ) distances_words = cosine_similarity ( word_embeddings , word_embeddings ) # Get 2*top_n words as candidates based on cosine similarity words_idx = list ( distances . argsort ()[ 0 ][ - nr_candidates :]) words_vals = [ words [ index ] for index in words_idx ] candidates = distances_words [ np . ix_ ( words_idx , words_idx )] # Calculate the combination of words that are the least similar to each other min_sim = 100_000 candidate = None for combination in itertools . combinations ( range ( len ( words_idx )), top_n ): sim = sum ([ candidates [ i ][ j ] for i in combination for j in combination if i != j ]) if sim < min_sim : candidate = combination min_sim = sim return [( words_vals [ idx ], round ( float ( distances [ 0 ][ idx ]), 4 )) for idx in candidate ]","title":"keybert.maxsum.max_sum_similarity"},{"location":"api/mmr.html","text":"Maximal Marginal Relevance \u00b6 \u00b6 Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return 5 diversity float How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.8 Returns: Type Description List[Tuple[str, float]] List[Tuple[str, float]]: The selected keywords/keyphrases with their distances Source code in keybert\\mmr.py def mmr ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int = 5 , diversity : float = 0.8 ) -> List [ Tuple [ str , float ]]: \"\"\" Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return diversity: How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. Returns: List[Tuple[str, float]]: The selected keywords/keyphrases with their distances \"\"\" # Extract similarity within words, and between words and the document word_doc_similarity = cosine_similarity ( word_embeddings , doc_embedding ) word_similarity = cosine_similarity ( word_embeddings ) # Initialize candidates and already choose best keyword/keyphras keywords_idx = [ np . argmax ( word_doc_similarity )] candidates_idx = [ i for i in range ( len ( words )) if i != keywords_idx [ 0 ]] for _ in range ( top_n - 1 ): # Extract similarities within candidates and # between candidates and selected keywords/phrases candidate_similarities = word_doc_similarity [ candidates_idx , :] target_similarities = np . max ( word_similarity [ candidates_idx ][:, keywords_idx ], axis = 1 ) # Calculate MMR mmr = ( 1 - diversity ) * candidate_similarities - diversity * target_similarities . reshape ( - 1 , 1 ) mmr_idx = candidates_idx [ np . argmax ( mmr )] # Update keywords & candidates keywords_idx . append ( mmr_idx ) candidates_idx . remove ( mmr_idx ) return [( words [ idx ], round ( float ( word_doc_similarity . reshape ( 1 , - 1 )[ 0 ][ idx ]), 4 )) for idx in keywords_idx ]","title":"MMR"},{"location":"api/mmr.html#maximal-marginal-relevance","text":"","title":"Maximal Marginal Relevance"},{"location":"api/mmr.html#keybert.mmr.mmr","text":"Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Parameters: Name Type Description Default doc_embedding ndarray The document embeddings required word_embeddings ndarray The embeddings of the selected candidate keywords/phrases required words List[str] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return 5 diversity float How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.8 Returns: Type Description List[Tuple[str, float]] List[Tuple[str, float]]: The selected keywords/keyphrases with their distances Source code in keybert\\mmr.py def mmr ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int = 5 , diversity : float = 0.8 ) -> List [ Tuple [ str , float ]]: \"\"\" Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return diversity: How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. Returns: List[Tuple[str, float]]: The selected keywords/keyphrases with their distances \"\"\" # Extract similarity within words, and between words and the document word_doc_similarity = cosine_similarity ( word_embeddings , doc_embedding ) word_similarity = cosine_similarity ( word_embeddings ) # Initialize candidates and already choose best keyword/keyphras keywords_idx = [ np . argmax ( word_doc_similarity )] candidates_idx = [ i for i in range ( len ( words )) if i != keywords_idx [ 0 ]] for _ in range ( top_n - 1 ): # Extract similarities within candidates and # between candidates and selected keywords/phrases candidate_similarities = word_doc_similarity [ candidates_idx , :] target_similarities = np . max ( word_similarity [ candidates_idx ][:, keywords_idx ], axis = 1 ) # Calculate MMR mmr = ( 1 - diversity ) * candidate_similarities - diversity * target_similarities . reshape ( - 1 , 1 ) mmr_idx = candidates_idx [ np . argmax ( mmr )] # Update keywords & candidates keywords_idx . append ( mmr_idx ) candidates_idx . remove ( mmr_idx ) return [( words [ idx ], round ( float ( word_doc_similarity . reshape ( 1 , - 1 )[ 0 ][ idx ]), 4 )) for idx in keywords_idx ]","title":"keybert.mmr.mmr"},{"location":"guides/embeddings.html","text":"Embedding Models \u00b6 The parameter model takes in a string pointing to a sentence-transformers model, a SentenceTransformer, or a Flair DocumentEmbedding model. Sentence-Transformers \u00b6 You can select any model from sentence-transformers here and pass it through KeyBERT with model : from keybert import KeyBERT model = KeyBERT ( model = 'distilbert-base-nli-mean-tokens' ) Or select a SentenceTransformer model with your own parameters: from keybert import KeyBERT from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" , device = \"cpu\" ) model = KeyBERT ( model = sentence_model ) Flair \u00b6 Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: from keybert import KeyBERT from flair.embeddings import TransformerDocumentEmbeddings roberta = TransformerDocumentEmbeddings ( 'roberta-base' ) model = KeyBERT ( model = roberta ) You can select any \ud83e\udd17 transformers model here .","title":"Embeddings"},{"location":"guides/embeddings.html#embedding-models","text":"The parameter model takes in a string pointing to a sentence-transformers model, a SentenceTransformer, or a Flair DocumentEmbedding model.","title":"Embedding Models"},{"location":"guides/embeddings.html#sentence-transformers","text":"You can select any model from sentence-transformers here and pass it through KeyBERT with model : from keybert import KeyBERT model = KeyBERT ( model = 'distilbert-base-nli-mean-tokens' ) Or select a SentenceTransformer model with your own parameters: from keybert import KeyBERT from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"distilbert-base-nli-mean-tokens\" , device = \"cpu\" ) model = KeyBERT ( model = sentence_model )","title":"Sentence-Transformers"},{"location":"guides/embeddings.html#flair","text":"Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: from keybert import KeyBERT from flair.embeddings import TransformerDocumentEmbeddings roberta = TransformerDocumentEmbeddings ( 'roberta-base' ) model = KeyBERT ( model = roberta ) You can select any \ud83e\udd17 transformers model here .","title":"Flair"},{"location":"guides/quickstart.html","text":"Installation \u00b6 Installation can be done using pypi : pip install keybert To use Flair embeddings, install KeyBERT as follows: pip install keybert [ flair ] Or to install all additional dependencies: pip install keybert [ all ] Usage \u00b6 The most minimal example can be seen below for the extraction of keywords: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" model = KeyBERT ( 'distilbert-base-nli-mean-tokens' ) keywords = model . extract_keywords ( doc ) You can set keyphrase_ngram_range to set the length of the resulting keywords/keyphrases: >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 1 ), stop_words = None ) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] To extract keyphrases, simply set keyphrase_ngram_range to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases: >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 2 ), stop_words = None ) [( 'learning algorithm' , 0.6978 ), ( 'machine learning' , 0.6305 ), ( 'supervised learning' , 0.5985 ), ( 'algorithm analyzes' , 0.5860 ), ( 'learning function' , 0.5850 )] NOTE : For a full overview of all possible transformer models see sentence-transformer . I would advise either 'distilbert-base-nli-mean-tokens' or 'xlm-r-distilroberta-base-paraphrase-v1' as they have shown great performance in semantic similarity and paraphrase identification respectively. Max Sum Similarity \u00b6 To diversity the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_maxsum = True , nr_candidates = 20 , top_n = 5 ) [( 'set training examples' , 0.7504 ), ( 'generalize training data' , 0.7727 ), ( 'requires learning algorithm' , 0.5050 ), ( 'supervised learning algorithm' , 0.3779 ), ( 'learning machine learning' , 0.2891 )] Maximal Marginal Relevance \u00b6 To diversify the results, we can use Maximal Margin Relevance (MMR) to create keywords / keyphrases which is also based on cosine similarity. The results with high diversity : >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_mmr = True , diversity = 0.7 ) [( 'algorithm generalize training' , 0.7727 ), ( 'labels unseen instances' , 0.1649 ), ( 'new examples optimal' , 0.4185 ), ( 'determine class labels' , 0.4774 ), ( 'supervised learning algorithm' , 0.7502 )] The results with low diversity : >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_mmr = True , diversity = 0.2 ) [( 'algorithm generalize training' , 0.7727 ), ( 'supervised learning algorithm' , 0.7502 ), ( 'learning machine learning' , 0.7577 ), ( 'learning algorithm analyzes' , 0.7587 ), ( 'learning algorithm generalize' , 0.7514 )]","title":"Quickstart"},{"location":"guides/quickstart.html#installation","text":"Installation can be done using pypi : pip install keybert To use Flair embeddings, install KeyBERT as follows: pip install keybert [ flair ] Or to install all additional dependencies: pip install keybert [ all ]","title":"Installation"},{"location":"guides/quickstart.html#usage","text":"The most minimal example can be seen below for the extraction of keywords: from keybert import KeyBERT doc = \"\"\" Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias). \"\"\" model = KeyBERT ( 'distilbert-base-nli-mean-tokens' ) keywords = model . extract_keywords ( doc ) You can set keyphrase_ngram_range to set the length of the resulting keywords/keyphrases: >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 1 ), stop_words = None ) [( 'learning' , 0.4604 ), ( 'algorithm' , 0.4556 ), ( 'training' , 0.4487 ), ( 'class' , 0.4086 ), ( 'mapping' , 0.3700 )] To extract keyphrases, simply set keyphrase_ngram_range to (1, 2) or higher depending on the number of words you would like in the resulting keyphrases: >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 1 , 2 ), stop_words = None ) [( 'learning algorithm' , 0.6978 ), ( 'machine learning' , 0.6305 ), ( 'supervised learning' , 0.5985 ), ( 'algorithm analyzes' , 0.5860 ), ( 'learning function' , 0.5850 )] NOTE : For a full overview of all possible transformer models see sentence-transformer . I would advise either 'distilbert-base-nli-mean-tokens' or 'xlm-r-distilroberta-base-paraphrase-v1' as they have shown great performance in semantic similarity and paraphrase identification respectively.","title":"Usage"},{"location":"guides/quickstart.html#max-sum-similarity","text":"To diversity the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity. >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_maxsum = True , nr_candidates = 20 , top_n = 5 ) [( 'set training examples' , 0.7504 ), ( 'generalize training data' , 0.7727 ), ( 'requires learning algorithm' , 0.5050 ), ( 'supervised learning algorithm' , 0.3779 ), ( 'learning machine learning' , 0.2891 )]","title":"Max Sum Similarity"},{"location":"guides/quickstart.html#maximal-marginal-relevance","text":"To diversify the results, we can use Maximal Margin Relevance (MMR) to create keywords / keyphrases which is also based on cosine similarity. The results with high diversity : >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_mmr = True , diversity = 0.7 ) [( 'algorithm generalize training' , 0.7727 ), ( 'labels unseen instances' , 0.1649 ), ( 'new examples optimal' , 0.4185 ), ( 'determine class labels' , 0.4774 ), ( 'supervised learning algorithm' , 0.7502 )] The results with low diversity : >>> model . extract_keywords ( doc , keyphrase_ngram_range = ( 3 , 3 ), stop_words = 'english' , use_mmr = True , diversity = 0.2 ) [( 'algorithm generalize training' , 0.7727 ), ( 'supervised learning algorithm' , 0.7502 ), ( 'learning machine learning' , 0.7577 ), ( 'learning algorithm analyzes' , 0.7587 ), ( 'learning algorithm generalize' , 0.7514 )]","title":"Maximal Marginal Relevance"}]}